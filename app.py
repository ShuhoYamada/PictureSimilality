from typing import Optional
import io

import streamlit as st
import numpy as np

from src.preprocessor import load_and_binarize, load_and_binarize_with_original, extract_contour, resample_contour
from src.visualizer import plot_contour


st.set_page_config(page_title="2æ¬¡å…ƒå½¢çŠ¶é¡ä¼¼åº¦è§£æ", layout="wide")
st.title("2æ¬¡å…ƒå½¢çŠ¶é¡ä¼¼åº¦è§£æ")

st.sidebar.header("å…¥åŠ›è¨­å®š")
# Allow multiple files for global view, but keep ability to inspect single image
uploaded_files = st.sidebar.file_uploader(
    "ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (TIFF/PNG/JPG) â€” Global/Local ä¸¡å¯¾å¿œ",
    accept_multiple_files=True,
    type=["tif", "tiff", "png", "jpg", "jpeg"],
)
threshold = st.sidebar.slider("é–¾å€¤ (0=Otsu)", 0, 255, 0)
epsilon_factor = st.sidebar.slider("approx epsilon factor", 0.001, 0.05, 0.01)
num_points = st.sidebar.slider("ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‚¹æ•°", 50, 1000, 200)
include_holes = st.sidebar.checkbox("ç©´ï¼ˆå†…å´è¼ªéƒ­ï¼‰ã‚’å«ã‚ã‚‹", value=True)
min_hole_area = st.sidebar.slider("ç©´ã®æœ€å°é¢ç© (pxÂ²)", 10, 1000, 100) if include_holes else 100
num_fourier = st.sidebar.slider("ãƒ•ãƒ¼ãƒªã‚¨ä¿‚æ•°æ•° (num_coeffs)", 4, 128, 16)
method = st.sidebar.selectbox("åŸ‹ã‚è¾¼ã¿æ‰‹æ³•", ["MDS", "TSNE"])

# ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è¨­å®š
st.sidebar.header("ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è¨­å®š")
enable_clustering = st.sidebar.checkbox("ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’æœ‰åŠ¹ã«ã™ã‚‹", value=False)
if enable_clustering:
    cluster_method = st.sidebar.selectbox(
        "ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•",
        ["K-means", "DBSCAN", "éšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°"],
        help="K-means: æŒ‡å®šã—ãŸæ•°ã®ã‚¯ãƒ©ã‚¹ã‚¿ã«åˆ†å‰²\nDBSCAN: å¯†åº¦ãƒ™ãƒ¼ã‚¹ï¼ˆè‡ªå‹•ã§ã‚¯ãƒ©ã‚¹ã‚¿æ•°æ±ºå®šï¼‰\néšå±¤çš„: éšå±¤çš„ã«çµ±åˆ"
    )
    if cluster_method == "K-means":
        n_clusters = st.sidebar.slider("ã‚¯ãƒ©ã‚¹ã‚¿æ•°", 2, 10, 3)
        cluster_params = {"method": "kmeans", "n_clusters": n_clusters}
    elif cluster_method == "DBSCAN":
        eps = st.sidebar.slider("è¿‘å‚åŠå¾„ (eps)", 0.1, 2.0, 0.5, 0.1)
        min_samples = st.sidebar.slider("æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°", 1, 10, 2)
        cluster_params = {"method": "dbscan", "eps": eps, "min_samples": min_samples}
    else:  # éšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        n_clusters = st.sidebar.slider("ã‚¯ãƒ©ã‚¹ã‚¿æ•°", 2, 10, 3)
        cluster_params = {"method": "hierarchical", "n_clusters": n_clusters}
else:
    cluster_params = None


@st.cache_data
def _process_image(file_bytes: bytes, threshold: int, epsilon_factor: float, num_points: int, include_holes: bool = True, min_hole_area: int = 100):
    """ç”»åƒã‚’å‡¦ç†ã—ã¦è¼ªéƒ­ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    
    Returns:
        include_holes=True ã®å ´åˆ: ((outer, holes, islands), binary)
        include_holes=False ã®å ´åˆ: (outer, binary)
    """
    binary = load_and_binarize(file_bytes, threshold if threshold > 0 else None)
    contour_data = extract_contour(binary, epsilon_factor, include_holes=include_holes, min_hole_area=min_hole_area)
    if contour_data is None:
        return None, binary
    
    if include_holes and isinstance(contour_data, tuple):
        # ã‚¿ãƒ—ãƒ«ã®å ´åˆ: (outer, holes, islands)
        outer, holes, islands = contour_data
        outer_resampled = resample_contour(outer, num_points)
        # ç©´ã¨å³¶ã‚‚ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆç‚¹æ•°ã¯æ¯”ä¾‹é…åˆ†ï¼‰
        holes_resampled = []
        for hole in holes:
            hole_pts = max(20, int(num_points * len(hole) / max(len(outer), 1)))
            holes_resampled.append(resample_contour(hole, hole_pts))
        islands_resampled = []
        for island in islands:
            island_pts = max(20, int(num_points * len(island) / max(len(outer), 1)))
            islands_resampled.append(resample_contour(island, island_pts))
        return (outer_resampled, holes_resampled, islands_resampled), binary
    else:
        # å˜ç´”ãªé…åˆ—ã®å ´åˆ
        resampled = resample_contour(contour_data, num_points)
        return resampled, binary


@st.cache_data
def _process_image_with_original(file_bytes: bytes, threshold: int, epsilon_factor: float, num_points: int, include_holes: bool = True, min_hole_area: int = 100):
    """å…ƒç”»åƒã‚‚ä¸€ç·’ã«è¿”ã™å‡¦ç†é–¢æ•°"""
    original, binary = load_and_binarize_with_original(file_bytes, threshold if threshold > 0 else None)
    contour_data = extract_contour(binary, epsilon_factor, include_holes=include_holes, min_hole_area=min_hole_area)
    if contour_data is None:
        return None, original, binary
    
    if include_holes and isinstance(contour_data, tuple):
        outer, holes, islands = contour_data
        outer_resampled = resample_contour(outer, num_points)
        holes_resampled = [resample_contour(h, max(20, int(num_points * len(h) / max(len(outer), 1)))) for h in holes]
        islands_resampled = [resample_contour(isl, max(20, int(num_points * len(isl) / max(len(outer), 1)))) for isl in islands]
        return (outer_resampled, holes_resampled, islands_resampled), original, binary
    else:
        resampled = resample_contour(contour_data, num_points)
        return resampled, original, binary


@st.cache_data
def _process_multiple(files_data: tuple, threshold: int, epsilon: float, num_points: int, num_fourier: int, method: str, include_holes: bool = True, min_hole_area: int = 100):
    """è¤‡æ•°ç”»åƒã‚’å‡¦ç†ã™ã‚‹ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ãƒ¼ã‚¿ã¯ãƒã‚¤ãƒˆåˆ—ã®ã‚¿ãƒ—ãƒ«ã¨ã—ã¦æ¸¡ã™ï¼‰"""
    from src.global_features import compute_global_embedding

    contours = {}
    skipped = []
    for name, data in files_data:
        try:
            contour, _ = _process_image(data, threshold if threshold > 0 else None, epsilon, num_points, include_holes, min_hole_area)
            if contour is None:
                skipped.append(name)
            else:
                contours[name] = contour
        except Exception:
            skipped.append(name)

    if len(contours) < 2:
        return None, skipped

    df, skipped_more = compute_global_embedding(contours, num_fourier=num_fourier, method=method)
    skipped += skipped_more
    return df, skipped


# ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒã™ã‚‹ãŸã‚ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥é–¢æ•°
@st.cache_data
def _get_contours_and_images(files_data: tuple, threshold: int, epsilon: float, num_points: int, include_holes: bool, min_hole_area: int):
    """è¼ªéƒ­ã¨å…ƒç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    
    Returns:
        contours: {name: contour_data} - contour_dataã¯(outer, holes, islands)ã‚¿ãƒ—ãƒ«ã¾ãŸã¯np.ndarray
        images: {name: original_image}
        skipped: list of skipped file names
    """
    contours = {}
    images = {}  # å…ƒç”»åƒã‚’ä¿æŒ
    skipped = []
    
    for name, data in files_data:
        try:
            original, binary = load_and_binarize_with_original(data, threshold if threshold > 0 else None)
            contour_data = extract_contour(binary, epsilon, include_holes=include_holes, min_hole_area=min_hole_area)
            if contour_data is None:
                skipped.append(name)
            else:
                # è¼ªéƒ­ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                if include_holes and isinstance(contour_data, tuple):
                    outer, holes, islands = contour_data
                    outer_resampled = resample_contour(outer, num_points)
                    holes_resampled = [resample_contour(h, max(20, int(num_points * len(h) / max(len(outer), 1)))) for h in holes]
                    islands_resampled = [resample_contour(isl, max(20, int(num_points * len(isl) / max(len(outer), 1)))) for isl in islands]
                    contours[name] = (outer_resampled, holes_resampled, islands_resampled)
                else:
                    contours[name] = resample_contour(contour_data, num_points)
                images[name] = original  # å…ƒç”»åƒã‚’ä¿å­˜
        except Exception:
            skipped.append(name)
    
    return contours, images, skipped


tabs = st.tabs(["Global Map", "Single Image", "Local Comparison", "é¡ä¼¼ç”»åƒæ¤œç´¢"])

# --- Global Map Tab
with tabs[0]:
    st.header("Global Map")
    if not uploaded_files or len(uploaded_files) < 2:
        st.info("è¤‡æ•°ã®ç”»åƒã‚’ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼ˆ2æšä»¥ä¸Šï¼‰ã€‚")
    else:
        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æƒ…å ±ã‚’è¡¨ç¤º
        st.info(f"ğŸ“ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒ: {len(uploaded_files)}æš")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
        if 'global_map_analyzed' not in st.session_state:
            st.session_state.global_map_analyzed = False
        if 'global_map_df' not in st.session_state:
            st.session_state.global_map_df = None
        if 'global_map_skipped' not in st.session_state:
            st.session_state.global_map_skipped = []
        
        # è§£æãƒœã‚¿ãƒ³
        col_btn1, col_btn2 = st.columns([1, 3])
        with col_btn1:
            analyze_global = st.button("ğŸ”¬ è§£æé–‹å§‹", type="primary", use_container_width=True, key="global_analyze")
        with col_btn2:
            if st.session_state.global_map_analyzed and st.session_state.global_map_df is not None:
                st.success(f"âœ… è§£ææ¸ˆã¿: {len(st.session_state.global_map_df)}æš")
        
        # è§£æãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸã‚‰å‡¦ç†ã‚’é–‹å§‹
        if analyze_global:
            st.session_state.global_map_analyzed = False
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            files_data = []
            for f in uploaded_files:
                f.seek(0)
                files_data.append((f.name, f.read()))
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã§é€²æ—ã‚’è¡¨ç¤º
            progress_bar = st.progress(0, text="ç”»åƒã‚’èª­ã¿è¾¼ã¿ä¸­...")
            status_text = st.empty()
            
            # è¼ªéƒ­ã‚’æŠ½å‡º
            contours = {}
            skipped = []
            total = len(files_data)
            
            for i, (name, data) in enumerate(files_data):
                try:
                    contour, _ = _process_image(data, threshold, epsilon_factor, num_points, include_holes, min_hole_area)
                    if contour is None:
                        skipped.append(name)
                    else:
                        contours[name] = contour
                except Exception:
                    skipped.append(name)
                
                if (i + 1) % 100 == 0 or i == total - 1:
                    progress = (i + 1) / total
                    progress_bar.progress(progress, text=f"ç”»åƒã‚’èª­ã¿è¾¼ã¿ä¸­... ({i + 1}/{total})")
            
            status_text.text(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(contours)}æš (ã‚¹ã‚­ãƒƒãƒ—: {len(skipped)}æš)")
            
            if len(contours) >= 2:
                # åŸ‹ã‚è¾¼ã¿è¨ˆç®—
                progress_bar.progress(0.5, text="åŸ‹ã‚è¾¼ã¿ã‚’è¨ˆç®—ä¸­...")
                from src.global_features import compute_global_embedding
                
                df, skipped_more = compute_global_embedding(contours, num_fourier=num_fourier, method=method)
                skipped.extend(skipped_more)
                
                # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’é©ç”¨
                if enable_clustering and cluster_params is not None:
                    progress_bar.progress(0.8, text="ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ä¸­...")
                    from src.global_features import cluster_shapes
                    df = cluster_shapes(df, **cluster_params)
                
                st.session_state.global_map_df = df
                st.session_state.global_map_skipped = skipped
                st.session_state.global_map_analyzed = True
                
                progress_bar.progress(1.0, text="å®Œäº†!")
                st.rerun()
            else:
                st.error("æœ‰åŠ¹ãªè¼ªéƒ­ã‚’æŒã¤ç”»åƒãŒ2æšä»¥ä¸Šè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        # è§£ææ¸ˆã¿ã®å ´åˆã€çµæœã‚’è¡¨ç¤º
        if st.session_state.global_map_analyzed and st.session_state.global_map_df is not None:
            df = st.session_state.global_map_df
            skipped = st.session_state.global_map_skipped
            
            st.markdown("---")
            
            from src.visualizer import plot_global_map
            fig = plot_global_map(df, show_clusters=enable_clustering)
            st.plotly_chart(fig, use_container_width=True)
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã‚’è¡¨ç¤º
            if enable_clustering and "cluster" in df.columns:
                st.subheader("ğŸ“Š ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœ")
                
                n_clusters = df["cluster"].nunique()
                noise_count = len(df[df["cluster"] == -1]) if -1 in df["cluster"].values else 0
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ã‚¯ãƒ©ã‚¹ã‚¿æ•°", n_clusters - (1 if noise_count > 0 else 0))
                with col2:
                    st.metric("åˆ†é¡æ¸ˆã¿", len(df) - noise_count)
                with col3:
                    if noise_count > 0:
                        st.metric("ãƒã‚¤ã‚ºï¼ˆæœªåˆ†é¡ï¼‰", noise_count)
                
                with st.expander("ğŸ—‚ï¸ ã‚¯ãƒ©ã‚¹ã‚¿åˆ¥ãƒ¡ãƒ³ãƒãƒ¼ä¸€è¦§"):
                    for cluster_id in sorted(df["cluster"].unique()):
                        if cluster_id == -1:
                            st.write("**ãƒã‚¤ã‚ºï¼ˆæœªåˆ†é¡ï¼‰:**")
                        else:
                            st.write(f"**Cluster {cluster_id}:**")
                        members = df[df["cluster"] == cluster_id]["label"].tolist()
                        st.write(", ".join(members))
                        st.markdown("---")
            
            with st.expander("ğŸ“Š åŸ‹ã‚è¾¼ã¿åº§æ¨™ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º"):
                st.dataframe(df)

            if skipped:
                st.warning(f"ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯è¼ªéƒ­æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ: {skipped}")

# --- Single Image Tab
with tabs[1]:
    st.header("Single Image Inspection")
    st.write("å˜ä¸€ã®ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦è¼ªéƒ­ã‚’ç¢ºèªã§ãã¾ã™ã€‚")
    single = st.file_uploader("å˜ä¸€ç”»åƒ (Single inspection)", accept_multiple_files=False, type=["tif", "tiff", "png", "jpg", "jpeg"], key="single")
    
    if single is None:
        st.info("ã“ã“ã§ã¯1æšã®ç”»åƒã‚’é¸ã‚“ã§è¼ªéƒ­ã‚’ç¢ºèªã§ãã¾ã™ã€‚")
    else:
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
        if 'single_analyzed' not in st.session_state:
            st.session_state.single_analyzed = False
        if 'single_result' not in st.session_state:
            st.session_state.single_result = None
        
        # è§£æãƒœã‚¿ãƒ³
        col_btn1, col_btn2 = st.columns([1, 3])
        with col_btn1:
            analyze_single = st.button("ğŸ”¬ è§£æé–‹å§‹", type="primary", use_container_width=True, key="single_analyze")
        with col_btn2:
            if st.session_state.single_analyzed:
                st.success("âœ… è§£ææ¸ˆã¿")
        
        if analyze_single:
            try:
                single.seek(0)
                file_bytes = single.read()
                contour, original, binary = _process_image_with_original(file_bytes, threshold if threshold > 0 else None, epsilon_factor, num_points, include_holes, min_hole_area)
                
                st.session_state.single_result = {
                    'contour': contour,
                    'original': original,
                    'binary': binary,
                    'name': single.name
                }
                st.session_state.single_analyzed = True
                st.rerun()
            except Exception as e:
                st.exception(e)
        
        # è§£ææ¸ˆã¿ã®å ´åˆã€çµæœã‚’è¡¨ç¤º
        if st.session_state.single_analyzed and st.session_state.single_result:
            result = st.session_state.single_result
            
            st.markdown("---")
            
            # å…ƒç”»åƒã¨å‡¦ç†å¾Œç”»åƒã‚’ä¸¦ã¹ã¦è¡¨ç¤º
            st.subheader("ç”»åƒæ¯”è¼ƒ")
            img_col1, img_col2 = st.columns(2)
            with img_col1:
                st.image(result['original'], caption="å…ƒç”»åƒ (Original)", use_container_width=True)
            with img_col2:
                st.image(result['binary'], caption="äºŒå€¤åŒ–ç”»åƒ (Binarized)", use_container_width=True)
            
            st.markdown("---")
            
            if result['contour'] is None:
                st.error("è¼ªéƒ­ãŒæ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚é–¾å€¤è¨­å®šã‚„ç”»åƒã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            else:
                st.subheader("æŠ½å‡ºã•ã‚ŒãŸè¼ªéƒ­")
                fig = plot_contour(result['contour'], title=result['name'])
                st.plotly_chart(fig, use_container_width=True)

# --- Local Comparison Tab
with tabs[2]:
    st.header("Local Comparison")
    st.write("2æšã®ç”»åƒã‚’é¸æŠã—ã¦ã€ä½ç½®åˆã‚ã›ï¼ˆé‡å¿ƒ + Procrustes / ICPï¼‰ã¨å·®åˆ†ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")

    col1, col2 = st.columns(2)
    with col1:
        ref = st.file_uploader("åŸºæº–ç”»åƒ (reference)", accept_multiple_files=False, type=["tif", "tiff", "png", "jpg", "jpeg"], key="ref")
    with col2:
        tgt = st.file_uploader("æ¯”è¼ƒç”»åƒ (target)", accept_multiple_files=False, type=["tif", "tiff", "png", "jpg", "jpeg"], key="tgt")

    icp_checkbox = st.checkbox("ICP ã«ã‚ˆã‚‹å¾®èª¿æ•´ã‚’è¡Œã†", value=True)
    run = st.button("è§£æé–‹å§‹ (Align & Compute)")

    if run:
        if ref is None or tgt is None:
            st.error("åŸºæº–ç”»åƒã¨æ¯”è¼ƒç”»åƒã®ä¸¡æ–¹ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
        else:
            try:
                b1 = ref.read()
                b2 = tgt.read()
                ref_contour, ref_orig, ref_bin = _process_image_with_original(b1, threshold if threshold > 0 else None, epsilon_factor, num_points, include_holes, min_hole_area)
                tgt_contour, tgt_orig, tgt_bin = _process_image_with_original(b2, threshold if threshold > 0 else None, epsilon_factor, num_points, include_holes, min_hole_area)

                # å…ƒç”»åƒã¨å‡¦ç†å¾Œç”»åƒã‚’ä¸¦ã¹ã¦è¡¨ç¤º
                st.subheader("å…¥åŠ›ç”»åƒã®æ¯”è¼ƒ")
                
                # åŸºæº–ç”»åƒ: å…ƒç”»åƒã¨äºŒå€¤åŒ–ç”»åƒ
                st.write(f"**åŸºæº–ç”»åƒ: {ref.name}**")
                ref_col1, ref_col2 = st.columns(2)
                with ref_col1:
                    st.image(ref_orig, caption="å…ƒç”»åƒ (Original)", use_container_width=True)
                with ref_col2:
                    st.image(ref_bin, caption="äºŒå€¤åŒ–ç”»åƒ (Binarized)", use_container_width=True)
                
                # æ¯”è¼ƒç”»åƒ: å…ƒç”»åƒã¨äºŒå€¤åŒ–ç”»åƒ
                st.write(f"**æ¯”è¼ƒç”»åƒ: {tgt.name}**")
                tgt_col1, tgt_col2 = st.columns(2)
                with tgt_col1:
                    st.image(tgt_orig, caption="å…ƒç”»åƒ (Original)", use_container_width=True)
                with tgt_col2:
                    st.image(tgt_bin, caption="äºŒå€¤åŒ–ç”»åƒ (Binarized)", use_container_width=True)
                
                st.markdown("---")

                if ref_contour is None or tgt_contour is None:
                    st.error("ã„ãšã‚Œã‹ã®ç”»åƒã§è¼ªéƒ­ãŒæ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚é–¾å€¤ã‚„ç”»åƒã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                else:
                    from src.local_analysis import align_shapes, compute_local_distance
                    from src.visualizer import plot_local_comparison

                    aligned_tgt, transform = align_shapes(tgt_contour, ref_contour, use_icp=icp_checkbox)

                    # Compute distances: source=ref, target=aligned_tgt
                    _, hausdorff, chamfer, target_to_source = compute_local_distance(ref_contour, aligned_tgt)

                    st.subheader("è¼ªéƒ­ã®ä½ç½®åˆã‚ã›çµæœ")
                    fig = plot_local_comparison(ref_contour, aligned_tgt, target_to_source, title=f"{ref.name} â†” {tgt.name}")
                    st.plotly_chart(fig, use_container_width=True)

                    st.markdown("---")
                    metric_col1, metric_col2 = st.columns(2)
                    with metric_col1:
                        st.metric("Hausdorff distance (px)", f"{hausdorff:.3f}")
                    with metric_col2:
                        st.metric("Chamfer mean (px)", f"{chamfer:.3f}")

                    with st.expander("è©³ç´°: å¤‰æ›è¡Œåˆ—ãƒ»å¤‰æ›é‡"):
                        st.write(transform)

            except Exception as e:
                st.exception(e)

# --- Similar Image Search Tab
with tabs[3]:
    st.header("é¡ä¼¼ç”»åƒæ¤œç´¢")
    st.write("ç”»åƒã‚’é¸æŠã™ã‚‹ã¨ã€ä¼¼ã¦ã„ã‚‹ç”»åƒã‚’é¡ä¼¼åº¦é †ã«è¡¨ç¤ºã—ã¾ã™ã€‚")
    
    if not uploaded_files or len(uploaded_files) < 2:
        st.info("è¤‡æ•°ã®ç”»åƒã‚’ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼ˆ2æšä»¥ä¸Šï¼‰ã€‚")
    else:
        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æƒ…å ±ã‚’è¡¨ç¤º
        st.info(f"ğŸ“ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒ: {len(uploaded_files)}æš")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
        if 'similarity_analyzed' not in st.session_state:
            st.session_state.similarity_analyzed = False
        if 'similarity_contours' not in st.session_state:
            st.session_state.similarity_contours = None
        if 'similarity_images' not in st.session_state:
            st.session_state.similarity_images = None
        if 'similarity_features' not in st.session_state:
            st.session_state.similarity_features = None
        if 'similarity_skipped' not in st.session_state:
            st.session_state.similarity_skipped = []
        
        # è§£æãƒœã‚¿ãƒ³
        col_btn1, col_btn2 = st.columns([1, 3])
        with col_btn1:
            analyze_button = st.button("ğŸ”¬ è§£æé–‹å§‹", type="primary", use_container_width=True)
        with col_btn2:
            if st.session_state.similarity_analyzed:
                st.success(f"âœ… è§£ææ¸ˆã¿: {len(st.session_state.similarity_contours)}æš")
        
        # è§£æãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸã‚‰å‡¦ç†ã‚’é–‹å§‹
        if analyze_button:
            st.session_state.similarity_analyzed = False  # ãƒªã‚»ãƒƒãƒˆ
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            files_data = []
            for f in uploaded_files:
                f.seek(0)
                files_data.append((f.name, f.read()))
            files_data_tuple = tuple(files_data)
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã§é€²æ—ã‚’è¡¨ç¤º
            progress_bar = st.progress(0, text="ç”»åƒã‚’èª­ã¿è¾¼ã¿ä¸­...")
            status_text = st.empty()
            
            # è¼ªéƒ­ã¨ç”»åƒã‚’å–å¾—ï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰
            contours = {}
            images = {}
            skipped = []
            total = len(files_data_tuple)
            
            for i, (name, data) in enumerate(files_data_tuple):
                try:
                    from src.preprocessor import load_and_binarize_with_original, extract_contour, resample_contour
                    original, binary = load_and_binarize_with_original(data, threshold if threshold > 0 else None)
                    contour_data = extract_contour(binary, epsilon_factor, include_holes=include_holes, min_hole_area=min_hole_area)
                    if contour_data is None:
                        skipped.append(name)
                    else:
                        if include_holes and isinstance(contour_data, tuple):
                            outer, holes, islands = contour_data
                            outer_resampled = resample_contour(outer, num_points)
                            holes_resampled = [resample_contour(h, max(20, int(num_points * len(h) / max(len(outer), 1)))) for h in holes]
                            islands_resampled = [resample_contour(isl, max(20, int(num_points * len(isl) / max(len(outer), 1)))) for isl in islands]
                            contours[name] = (outer_resampled, holes_resampled, islands_resampled)
                        else:
                            contours[name] = resample_contour(contour_data, num_points)
                        images[name] = original
                except Exception:
                    skipped.append(name)
                
                # é€²æ—æ›´æ–°ï¼ˆ100ä»¶ã”ã¨ï¼‰
                if (i + 1) % 100 == 0 or i == total - 1:
                    progress = (i + 1) / total
                    progress_bar.progress(progress, text=f"ç”»åƒã‚’èª­ã¿è¾¼ã¿ä¸­... ({i + 1}/{total})")
            
            status_text.text(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(contours)}æš (ã‚¹ã‚­ãƒƒãƒ—: {len(skipped)}æš)")
            
            if len(contours) >= 2:
                # ç‰¹å¾´é‡ã‚’è¨ˆç®—
                progress_bar.progress(0, text="ç‰¹å¾´é‡ã‚’è¨ˆç®—ä¸­...")
                from src.global_features import compute_all_features
                
                features = {}
                total_contours = len(contours)
                for i, (name, contour) in enumerate(contours.items()):
                    try:
                        from src.global_features import compute_feature_vector
                        feat = compute_feature_vector(contour, num_fourier, use_holes=True)
                        if feat is not None:
                            features[name] = feat
                    except Exception:
                        pass
                    
                    if (i + 1) % 100 == 0 or i == total_contours - 1:
                        progress = (i + 1) / total_contours
                        progress_bar.progress(progress, text=f"ç‰¹å¾´é‡ã‚’è¨ˆç®—ä¸­... ({i + 1}/{total_contours})")
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
                st.session_state.similarity_contours = contours
                st.session_state.similarity_images = images
                st.session_state.similarity_features = features
                st.session_state.similarity_skipped = skipped
                st.session_state.similarity_analyzed = True
                
                progress_bar.progress(1.0, text="å®Œäº†!")
                st.success(f"âœ… è§£æå®Œäº†: {len(contours)}æšã®ç”»åƒã‹ã‚‰{len(features)}å€‹ã®ç‰¹å¾´é‡ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
                st.rerun()
            else:
                st.error("æœ‰åŠ¹ãªè¼ªéƒ­ã‚’æŒã¤ç”»åƒãŒ2æšä»¥ä¸Šè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        # è§£ææ¸ˆã¿ã®å ´åˆã€çµæœã‚’è¡¨ç¤º
        if st.session_state.similarity_analyzed and st.session_state.similarity_contours:
            contours = st.session_state.similarity_contours
            images = st.session_state.similarity_images
            precomputed_features = st.session_state.similarity_features
            
            st.markdown("---")
            
            # æ¤œç´¢å¯¾è±¡ã®ç”»åƒã‚’é¸æŠ
            available_images = list(contours.keys())
            
            col1, col2 = st.columns([1, 2])
            
            with col1:
                st.subheader("ğŸ” æ¤œç´¢å¯¾è±¡ã‚’é¸æŠ")
                query_image = st.selectbox(
                    "åŸºæº–ç”»åƒ",
                    available_images,
                    key="query_select"
                )
                
                top_k = st.slider("è¡¨ç¤ºã™ã‚‹é¡ä¼¼ç”»åƒæ•°", 1, min(10, len(available_images) - 1), 5)
                
                # é¸æŠã—ãŸç”»åƒã‚’è¡¨ç¤º
                if query_image and query_image in images:
                    st.image(images[query_image], caption=f"é¸æŠä¸­: {query_image}", use_container_width=True)
            
            with col2:
                if query_image:
                    from src.global_features import find_similar_shapes
                    
                    # é¡ä¼¼ç”»åƒã‚’æ¤œç´¢ï¼ˆäº‹å‰è¨ˆç®—æ¸ˆã¿ç‰¹å¾´é‡ã‚’ä½¿ç”¨ï¼‰
                    similar = find_similar_shapes(
                        query_image, contours, num_fourier, top_k,
                        precomputed_features=precomputed_features
                    )
                    
                    if similar:
                        st.subheader(f"ğŸ“Š é¡ä¼¼ç”»åƒ TOP {len(similar)}")
                        
                        # ã‚°ãƒªãƒƒãƒ‰è¡¨ç¤º
                        cols_per_row = 3
                        for i in range(0, len(similar), cols_per_row):
                            cols = st.columns(cols_per_row)
                            for j, col in enumerate(cols):
                                idx = i + j
                                if idx < len(similar):
                                    name, score = similar[idx]
                                    with col:
                                        if name in images:
                                            st.image(images[name], use_container_width=True)
                                        st.markdown(f"**{idx + 1}. {name}**")
                                        st.progress(score, text=f"é¡ä¼¼åº¦: {score:.1%}")
                    else:
                        st.warning("é¡ä¼¼ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            
            # é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            st.markdown("---")
            st.subheader("ğŸ“ˆ é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹")
            
            num_images = len(contours)
            st.info(f"ğŸ“Š å¯¾è±¡ç”»åƒæ•°: {num_images}æš")
            
            if num_images > 500:
                st.warning(f"âš ï¸ ç”»åƒæ•°ãŒå¤šã„ãŸã‚ï¼ˆ{num_images}æšï¼‰ã€ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—è¡¨ç¤ºã¯æœ€å¤§100æšã®ã‚µãƒ³ãƒ—ãƒ«ã«åˆ¶é™ã•ã‚Œã¾ã™ã€‚å®Œå…¨ãªãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã¯CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚")
            
            col_matrix1, col_matrix2 = st.columns(2)
            
            with col_matrix1:
                show_heatmap = st.checkbox("ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’è¡¨ç¤º", value=num_images <= 100)
            
            with col_matrix2:
                max_heatmap_samples = st.slider(
                    "ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°",
                    min_value=20,
                    max_value=200,
                    value=100,
                    help="ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã«è¡¨ç¤ºã™ã‚‹æœ€å¤§ç”»åƒæ•°ã€‚ãƒ¡ãƒ¢ãƒªã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãŸã‚åˆ¶é™ã—ã¦ã„ã¾ã™ã€‚"
                )
            
            from src.global_features import compute_pairwise_similarity, export_full_similarity_matrix
            
            # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—è¡¨ç¤º
            if show_heatmap:
                with st.spinner(f"é¡ä¼¼åº¦ã‚’è¨ˆç®—ä¸­... ({min(num_images, max_heatmap_samples)}æš)"):
                    progress_bar = st.progress(0)
                    
                    def update_progress(p):
                        progress_bar.progress(min(p, 1.0))
                    
                    sim_matrix, was_sampled = compute_pairwise_similarity(
                        contours, num_fourier, 
                        max_samples=max_heatmap_samples,
                        progress_callback=update_progress
                    )
                    progress_bar.empty()
                
                if not sim_matrix.empty:
                    if was_sampled:
                        st.info(f"ğŸ² è¡¨ç¤ºç”¨ã«{max_heatmap_samples}æšã‚’ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¾ã—ãŸã€‚")
                    
                    import plotly.express as px
                    
                    fig = px.imshow(
                        sim_matrix.values,
                        x=sim_matrix.columns,
                        y=sim_matrix.index,
                        color_continuous_scale="RdYlGn",
                        aspect="auto",
                        title="é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ï¼ˆç·‘=é¡ä¼¼ã€èµ¤=éé¡ä¼¼ï¼‰"
                    )
                    fig.update_layout(
                        xaxis_title="",
                        yaxis_title="",
                        xaxis=dict(tickangle=45),
                        height=max(400, min(800, len(sim_matrix) * 5))
                    )
                    st.plotly_chart(fig, use_container_width=True)
            
            # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå®Œå…¨ç‰ˆï¼‰
            st.markdown("### ğŸ“¥ å®Œå…¨ãªé¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
            
            if st.button("é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã‚’ç”Ÿæˆï¼ˆCSVï¼‰", key="generate_full_matrix"):
                with st.spinner(f"å…¨{num_images}æšã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ä¸­...ï¼ˆå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã€æ•°åˆ†ã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ï¼‰"):
                    csv_bytes = export_full_similarity_matrix(contours, num_fourier)
                
                if csv_bytes:
                    st.download_button(
                        "ğŸ“¥ CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        csv_bytes,
                        "similarity_matrix_full.csv",
                        "text/csv",
                        key="download_full_matrix"
                    )
                    st.success(f"âœ… {num_images}x{num_images}ã®é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚")
            
            if skipped:
                st.warning(f"ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯è¼ªéƒ­æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ: {skipped}")